{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08bcba96-0388-449e-895e-f7f25a3161f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ### Week 3 â€“ Day 3 : File Handling (CSV & JSON)\n",
    "\n",
    "Today weâ€™ll learn how to read and write data using Pythonâ€™s built-in file-handling modules.  \n",
    "This is the foundation of real ETL work where youâ€™ll interact with raw data files every day.\n",
    "\n",
    "**Learning Goals ðŸŽ¯**\n",
    "- Open, read, and write text files (`.txt`)\n",
    "- Work with CSV files using the `csv` module\n",
    "- Parse and update JSON data using the `json` module\n",
    "- Use `with open()` context managers to automate file closing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79fc7669-b243-4334-a386-875e5686cd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Contents:\n",
      " Python File Handling Basics\n",
      "Week 3 Day 3 Example\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1 â€” Writing and Reading a Text File\n",
    "file_path = \"../../datasets/sample_notes.txt\"\n",
    "\n",
    "\n",
    "# Write to a file\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(\"Python File Handling Basics\\n\")\n",
    "    f.write(\"Week 3 Day 3 Example\\n\")\n",
    "\n",
    "# Read the file back\n",
    "with open(file_path, \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(\"File Contents:\\n\", content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7019a9af-9b14-46e0-8210-74dbc1d04069",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "- `open(file, mode)` opens a file (`\"w\"` for write, `\"r\"` for read, `\"a\"` for append).  \n",
    "- `with open()` is a **context manager** that automatically closes the file after use.  \n",
    "- `f.write()` adds text to a file, and `f.read()` retrieves its contents.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a086f7c5-e3ef-4c98-847b-ea2a696a089a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['order_id', 'region', 'country', 'product', 'quantity', 'price', 'order_date', 'customer_id']\n",
      "['1', 'North America', 'USA', 'Laptop', '2', '899.99', '2024-01-10', '1']\n",
      "['2', 'Europe', 'Germany', 'Phone', '5', '499.99', '2024-01-12', '2']\n",
      "['3', 'Asia', 'India', 'Tablet', '3', '299.99', '2024-01-15', '3']\n",
      "['4', 'North America', 'Canada', 'Monitor', '4', '199.99', '2024-01-18', '']\n",
      "['5', 'Europe', 'France', 'Laptop', '1', '999.99', '2024-01-20', '4']\n",
      "['6', 'Asia', 'Japan', 'Phone', '6', '699.99', '2024-01-22', '5']\n",
      "['7', 'Europe', 'Germany', 'Monitor', '2', '199.99', '2024-01-23', '2']\n",
      "['8', 'North America', 'USA', 'Tablet', '4', '299.99', '2024-01-25', '1']\n"
     ]
    }
   ],
   "source": [
    "#Example 2 â€” Reading a CSV File\n",
    "import csv\n",
    "\n",
    "csv_path = \"../../datasets/sales.csv\"\n",
    "\n",
    "with open(csv_path, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571cba5e-02cb-44ff-944b-2eb4dd10b3e2",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "In this example, we use Pythonâ€™s built-in **`csv`** module to read structured tabular data from a `.csv` file.\n",
    "\n",
    "A CSV (Comma-Separated Values) file stores data in plain text form, where each line represents a record and each value is separated by commas.\n",
    "\n",
    "Weâ€™ll:\n",
    "1. Open the CSV file located in the `datasets/` folder  \n",
    "2. Use the `csv.reader()` function to iterate over each row  \n",
    "3. Print each row to understand how the data looks before doing any processing\n",
    "\n",
    " *In real data engineering projects, CSV files are one of the most common formats exchanged between systems (e.g., exports from databases, logs, or ETL outputs).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af3dc098-131b-4776-94de-7ebcc9970310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " {'name': 'Lakshmi Sharath', 'role': 'Data Engineer in Training', 'skills': ['Python', 'SQL', 'ETL']}\n",
      "\n",
      "Updated JSON saved to: ../../datasets/profile_updated.json\n",
      "Updated Data:\n",
      " {'name': 'Lakshmi Sharath', 'role': 'Data Engineer in Training', 'skills': ['Python', 'SQL', 'ETL', 'Pandas'], 'certifications': ['AWS Data Engineer', 'CPHQ']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_path = \"../../datasets/profile.json\"\n",
    "\n",
    "# Read JSON\n",
    "with open(json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "\n",
    "# Modify JSON\n",
    "data[\"skills\"].append(\"Pandas\")\n",
    "data[\"certifications\"] = [\"AWS Data Engineer\", \"CPHQ\"]\n",
    "\n",
    "# Write new JSON\n",
    "updated_json_path = \"../../datasets/profile_updated.json\"\n",
    "with open(updated_json_path, \"w\") as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "print(\"\\nUpdated JSON saved to:\", updated_json_path)\n",
    "print(\"Updated Data:\\n\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f5a8e-173d-426d-8545-cd44c001e3e8",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "JSON files store structured data as key-value pairs.  \n",
    "Weâ€™ll read, update, and save data using Pythonâ€™s `json` module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c31aec-2297-4d73-8957-365487595b95",
   "metadata": {},
   "source": [
    "# Explanation â€” Dataset File Handling\n",
    "Weâ€™re using the **datasets** folder as a single source for all raw input files.  \n",
    "Each path starts with `../../datasets/`, which tells Jupyter to move up two levels (from week3 â†’ root) and into the datasets directory.\n",
    "\n",
    "This structure:\n",
    "- Keeps data organized outside my notebooks  \n",
    "- Prevents accidental overwriting  \n",
    "- Matches real-world data engineering pipelines where data lives in a central storage layer  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c70ae-8fca-4295-9831-cdf166be0a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
